{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify location of `msd_summary_file.h5` or the whole `*.tar.gz` file here!\n",
    "\n",
    "# location = '../millionsongsubset_full.tar.gz'\n",
    "location = '../msd_summary_file.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports:\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import itertools as it\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DuplicateDetector class\n",
    "This class largely consists of boilerplate code, such as the initialization routine and the possibility for the user to change parameters like the amount `N` of dataset items to include, the rows `r` and bins `b` for the hash function to use, which features to include in the comparison, and what cutoff to use for the cosine similarity. There are also two simple methods `naive_cosine_similarity()` and `check_pairs()` that loop through all pairs or candidate pairs, respectively, and call the method `cosine_similarity(pair)`, which simply returns the dot product of the pair divided by the product of their norms. \n",
    "\n",
    "The important part of the class is contained in the three methods described in more detail here:\n",
    "\n",
    "### generate_signature_matrix()\n",
    "A pseudo signature matrix is computed by generating `r*b` random vectors and multiplying them with the data matrix. We don't actually care about the magnitude of these values, so the sign is computed. For simplicity, negative values are also converted to zero, so that our signature matrix consists of {0,1} entries rather than {-1,1} entries (This also eliminates the miniscule risk of getting a ternary matrix with {-1,0,1} entries from the sign function).\n",
    "\n",
    "Finally, we want to split the signature matrix into b bins, so it's reshaped from an `N` x `(r*b)` matrix into a `b` x `N` x `r` tensor, consisting of `b` different `N` x `r` sub-matrices - one for each bin.\n",
    "\n",
    "### get_candidate_pairs()\n",
    "This method iterates through all `b` bins and finds samples that have an identical hash within the bin. In order to achieve this quickly, numpy routines are used, instead of iterating through the whole array. As this is a bit tricky and involves the manipulation of data types for the sake of memory level access (something more associated with C than with Python programming), the individual steps are described here.\n",
    "\n",
    "1. ```python \n",
    "sub_matrix = np.ascontiguousarray(self.sm[b]).view(np.dtype((np.void, self.sm.dtype.itemsize * self.r)))\n",
    "```\n",
    "First create a local copy of the `b`-th sub-matrix, which is an `N` x `r` array of 8-bit unsigned integers. By calling the `view(dtype)` method, we are telling numpy to instead view it as a 1-dimensional size `N` array of (`8*r`)-bit voids. The reason for this is straightforward: numpy has an extremely efficient implementation for sorting and finding duplicates in `numpy.unique()`, but this only works on 1-dimensional or flattened arrays.\n",
    "\n",
    "2. ```python\n",
    "_, inverses = np.unique(sub_matrix, return_inverse=True)\n",
    "```\n",
    "Using the `return_inverse` flag, `numpy.unique()` returns not only an array `_` of all unique entries, but also an array `inverses` indexing which unique entry appears at each position in the original array. Mathematically speaking, this is a bijective function $\\mathcal{F} : \\mathbb{B}^r \\rightarrow \\mathbb{N}_0$. We are transforming from `r`-dimensional Boolean vectors to sequential integers, which are easier to work with. Effectively, `inverses` is a hash of our hash, so that it fits into convential data types.\n",
    "\n",
    "3. ```python\n",
    "sorted_args = np.argsort(inverses)\n",
    "```\n",
    "If we argsort this array, we now have the indices of our samples (integers from 0 to `N-1`) arranged in an order so that all samples with identical hashes are adjecent to each other.\n",
    "\n",
    "4. ```python\n",
    "sub_matrix_sorted = inverses[sorted_args]\n",
    "_, indices = np.unique(sub_matrix_sorted, return_index=True)\n",
    "candidate_tuples = np.split(sorted_args, indices[1:])\n",
    "```\n",
    "This array of `sorted_args` is now split up into several sub-arrays, each containing only samples with identical hashes. These subarrays could be of size 1 (no other matching sample), size 2 (exactly two matching samples), or size >2 (many samples all with the same hash).\n",
    "\n",
    "5. ```python\n",
    "for q in candidate_tuples:\n",
    "    if q.size > 2:\n",
    "        candidate_pairs.extend([np.asarray(g) for g in it.combinations(q, 2)])\n",
    "    elif q.size == 2:\n",
    "        candidate_pairs.append(q)\n",
    "```\n",
    "So finally we loop through these these tuples, discarding the unmatched samples, and using `itertools.combinations()` to generate all pairwise combinations within the >2 tuples. This concludes the generation of candidate pairs for a single bin `b`.\n",
    "\n",
    "### prune_duplicate_pairs(candidate_pairs)\n",
    "\n",
    "As we treated each bin totally separately, it is possible that some candidate pairs were detected in several bins. The last step before returning the candidate pairs is to prune duplicates:\n",
    "\n",
    "```python\n",
    "pc = np.ascontiguousarray(np.sort(candidate_pairs), dtype='uint32').view(dtype='uint64')\n",
    "self.candidate_pairs = np.unique(pc).view(dtype='uint32').reshape(-1, 2)\n",
    "```\n",
    "This uses a similar trick to above. The candidate pairs are saved as a `P` x `2` array of 32-bit unsigned integers (with `P` being the amount of pairs found), but numpy is told to view them as a 1-D size `P` array of 64-bit unsigned integers. Duplicates can now be trivially and efficiently removed using `numpy.unique()` and then the resulting shorter array is transformed and reshaped back to a `Q` x `2` array of 32-bit uints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DuplicateDetector:\n",
    "    \"\"\"\n",
    "    Class to detect duplicates in the million song dataset.\n",
    "\n",
    "    Accepts user parameters:\n",
    "    int N: amount of items from the dataset to consider\n",
    "    int r: amount of rows per bin of the hash function (must be below 64 due to data representation)\n",
    "    int b: amount of bins in the hash function\n",
    "    list of strings features: the names (keys) of the features to include in the similarity analysis\n",
    "    string location: the location of the .tar.gz archive containing the songs\n",
    "    float epsilon: the maximum angle between vectors that will be considered a duplicate\n",
    "    float distance: maximum cosine distance for duplicates (if both epsilon and distance are specified,\n",
    "                    distance takes priority.\n",
    "\n",
    "    Allows the following methods:\n",
    "    add_feature(feature): adds an extra feature to the list of features, reloads the data matrix\n",
    "    remove_feature(feature): removes one of the features from the list of features, reloads the data matrix\n",
    "    set_epsilon(epsilon): change the cutoff angle for the similarity comparison\n",
    "    set_distance(distance): change the cutoff cosine distance for the similarity comparison\n",
    "    generate_signature_matrix(): generates the signature matrix with random hyperplanes\n",
    "    get_candidate_pairs(): generates all candidate pairs that match from the hashing of the signature matrix\n",
    "    check_pairs(): checks all of the candidate pairs to compute the true cosine similarity, selects those <epsilon\n",
    "    list naive_cosine_similarity(): like above, but checks all possible pairs, without any hashing.\n",
    "    \"\"\"\n",
    "\n",
    "    ### Initialization and User Interface functions:\n",
    "\n",
    "    def __init__(self, N=10000, r=64, b=3, features=[], location='./millionsongsubset_full.tar.gz',\n",
    "                 epsilon=None, distance=None):\n",
    "        self.location = location\n",
    "        self.r = r\n",
    "        self.b = b\n",
    "        self.N = N\n",
    "        self.overhead_table = self.get_overhead_table()\n",
    "        self.features = []\n",
    "        for feature in features:\n",
    "            if self.legitimate_feature(feature) and feature not in self.features:\n",
    "                self.features.append(feature)\n",
    "        if not self.features:\n",
    "            self.features = ['duration', 'end_of_fade_in', 'key', 'loudness', 'mode',\n",
    "                             'start_of_fade_out', 'tempo', 'time_signature']\n",
    "        if len(self.overhead_table[self.features[0]]) < self.N:\n",
    "            self.N = len(self.overhead_table[self.features[0]])\n",
    "            print('maximum N value exceeded, correcting to {}.'.format(self.N))\n",
    "        self.data_matrix = self.get_data_matrix()\n",
    "        self.actual_pairs = []\n",
    "        if not (epsilon and distance):\n",
    "            self.cutoff = np.cos(5.0*np.pi/180.0)\n",
    "        if distance:\n",
    "            self.cutoff = 1 - distance\n",
    "        if epsilon:\n",
    "            self.cutoff = np.cos(epsilon * np.pi / 180.0)\n",
    "\n",
    "    def get_data_matrix(self):\n",
    "        M = np.zeros([self.N, len(self.features)])\n",
    "        for f, feature in enumerate(self.features):\n",
    "            M[:, f] = self.overhead_table[feature][:self.N]\n",
    "        return preprocessing.scale(M)\n",
    "\n",
    "    def get_overhead_table(self):\n",
    "        if self.location.endswith('.tar.gz'):\n",
    "            tar = tarfile.open(self.location, 'r')\n",
    "            members = tar.getmembers()\n",
    "            tar.extract(members[5])\n",
    "            summary = pd.HDFStore(members[5].name)\n",
    "            return summary['/analysis/songs']\n",
    "        elif self.location.endswith('.h5'):\n",
    "            summary = pd.HDFStore(self.location)\n",
    "            return summary['/analysis/songs']\n",
    "\n",
    "    def reload_data(self):\n",
    "        self.data_matrix = self.get_data_matrix()\n",
    "\n",
    "    def add_feature(self, feature):\n",
    "        if self.legitimate_feature(feature) and feature not in self.features:\n",
    "            self.features.append(feature)\n",
    "            self.reload_data()\n",
    "\n",
    "    def remove_feature(self, feature):\n",
    "        if feature in self.features:\n",
    "            self.features.remove(feature)\n",
    "            self.reload_data()\n",
    "\n",
    "    def legitimate_feature(self, feature):\n",
    "        return True if feature in self.overhead_table.keys() else False\n",
    "\n",
    "    def set_epsilon(self, epsilon):\n",
    "        self.cutoff = np.cos(epsilon * np.pi / 180.0)\n",
    "\n",
    "    def set_distance(self, distance):\n",
    "        self.cutoff = 1 - distance\n",
    "\n",
    "    def set_N(self, N):\n",
    "        self.N = N\n",
    "        if len(self.overhead_table[self.features[0]]) < self.N:\n",
    "            self.N = len(self.overhead_table[self.features[0]])\n",
    "            print('maximum N value exceeded, correcting to {}.'.format(self.N))\n",
    "        self.reload_data()\n",
    "\n",
    "    ### Similarity / Distance functions:\n",
    "\n",
    "    def generate_signature_matrix(self):\n",
    "        # multiply features with random hyperplanes:\n",
    "        signature_matrix = self.data_matrix.dot(np.random.normal(0, 1, [len(self.features), self.r*self.b]))\n",
    "        # convert all non-positive values to 0, all positive values to 1:\n",
    "        binary_signature_matrix = np.maximum(np.sign(signature_matrix), 0, signature_matrix).astype('uint8')\n",
    "\n",
    "        # reshape the \"N x (r*b)\" signature matrix into a \"b x N x r\" signature tensor (consisting of b separate\n",
    "        # signature sub-matrices that we can individually search for duplicates):\n",
    "        self.sm = np.reshape(binary_signature_matrix, [self.N, self.b, self.r]).transpose(1, 0, 2)\n",
    "\n",
    "    def get_candidate_pairs(self):\n",
    "        candidate_pairs = []\n",
    "        # loop through our b different sub-matrices:\n",
    "        for b in range(self.b):\n",
    "\n",
    "            # find all identical entries from the b'th signature sub-matrix using some dtype trickery:\n",
    "            sub_matrix = np.ascontiguousarray(self.sm[b]).view(np.dtype((np.void, self.sm.dtype.itemsize * self.r)))\n",
    "            _, inverses = np.unique(sub_matrix, return_inverse=True)\n",
    "            sorted_args = np.argsort(inverses)\n",
    "            sub_matrix_sorted = inverses[sorted_args]\n",
    "            _, indices = np.unique(sub_matrix_sorted, return_index=True)\n",
    "            candidate_tuples = np.split(sorted_args, indices[1:])\n",
    "\n",
    "            # each entry of candidate_tuples contains the indices of samples with the same hash. The size of the entries\n",
    "            # may be anywhere between 1 and several hundred, so throw out the size=1 entries, and expand the size>2\n",
    "            # entries to all pairwise combinations of their members:\n",
    "            for q in candidate_tuples:\n",
    "                if q.size > 2:\n",
    "                    candidate_pairs.extend([np.asarray(g) for g in it.combinations(q, 2)])\n",
    "                elif q.size == 2:\n",
    "                    candidate_pairs.append(q)\n",
    "        self.prune_duplicate_pairs(candidate_pairs)\n",
    "\n",
    "    def prune_duplicate_pairs(self, candidate_pairs):\n",
    "        # eliminate duplicate detections (that were marked identical from several sub-matrices)\n",
    "        pc = np.ascontiguousarray(np.sort(candidate_pairs), dtype='uint32').view(dtype='uint64')\n",
    "        self.candidate_pairs = np.unique(pc).view(dtype='uint32').reshape(-1, 2)\n",
    "\n",
    "    def cosine_similarity(self, pair):\n",
    "        return np.dot(self.data_matrix[pair[0]], self.data_matrix[pair[1]]) / (np.linalg.norm(self.data_matrix[pair[0]]) * np.linalg.norm(self.data_matrix[pair[1]]))\n",
    "\n",
    "    ### Simple for-loop functions computing the similarity between samples:\n",
    "\n",
    "    def check_pairs(self):\n",
    "        for pair in self.candidate_pairs:\n",
    "            if self.cosine_similarity(pair) > self.cutoff:\n",
    "                self.actual_pairs.append(pair)\n",
    "        self.actual_pairs = np.asarray(self.actual_pairs)\n",
    "\n",
    "    def naive_cosine_similarity(self):\n",
    "        naive_pairs = []\n",
    "        for n in range(self.N):\n",
    "            for m in range(n+1, self.N):\n",
    "                if self.cosine_similarity([n, m]) > self.cutoff:\n",
    "                    naive_pairs.append([n, m])\n",
    "        return naive_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated signature matrix, found 4256 candidate pairs in 0.13s.\n",
      "Found 23 actual pairs in 0.053s.\n"
     ]
    }
   ],
   "source": [
    "# now let's see how fast it is:\n",
    "\n",
    "import time\n",
    "\n",
    "dd = DuplicateDetector(N=10000, r=64, b=3, epsilon=2, location=location)\n",
    "\n",
    "start = time.time()\n",
    "dd.generate_signature_matrix()\n",
    "dd.get_candidate_pairs()\n",
    "print('Generated signature matrix, found {} candidate pairs in {:.2}s.'.format(len(dd.candidate_pairs), time.time()-start))\n",
    "\n",
    "start = time.time()\n",
    "dd.check_pairs()\n",
    "print('Found {} actual pairs in {:.2}s.'.format(len(dd.actual_pairs), time.time()-start))\n",
    "\n",
    "'''\n",
    "As reference, on a MacBook Air with i7 processor, the code obtains the results:\n",
    "> Generated signature matrix, found 4256 candidate pairs in 0.13s.\n",
    "> Found 23 actual pairs in 0.053s.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 23 actual pairs naively in 607.0s.\n"
     ]
    }
   ],
   "source": [
    "# for comparison, the naive pairwise comparison of everything:\n",
    "\n",
    "start = time.time()\n",
    "p = dd.naive_cosine_similarity()\n",
    "print('found {} actual pairs naively in {:.4}s.'.format(len(p), time.time()-start))\n",
    "\n",
    "'''\n",
    "As reference, on a MacBook Air with i7 processor, the code obtains the results:\n",
    "> found 23 actual pairs naively in 607.0s.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
